Music as More Than Sound: Developing Interactive Visual Representations of Audio InputsBen LerudeAdvisors: Scott Petersen and Holly RushmeierComputing and the Arts Senior Project: Final ReportYale University1.	ABSTRACTAs a Computing and the Arts major in the music track, I have spent the last few years learning about how music intersects with the world of computer science. Music cannot exist in a vacuum; it is constantly morphed and inspired by the world in which it exists, and as technology has changed the ways we live our everyday lives, so too has it changed the ways in which we create and consume music. The interdisciplinary nature of the major has inspired me to look for further connections between not only music and technology, but between even more fields as well. Music is naturally linked to other art forms, in particular visual media. The goal of this project was to create a musical system using the programming languages of Processing and SuperCollider in which sounds are represented visually. SuperCollider serves as the ideal language to process the music side of the system, and it also links well with the functionality of Processing, where the graphic displays are programmed. Different musical inputs, controlled by the user typing on a computer or MIDI keyboard, are interpreted by the system and activate corresponding visual responses that depict the musical data obtained by the live performance. The visualizations show two- and three-dimensional graphs that illustrate the pitches, chords, and harmonies played by the user. The intent is to think of these attributes of music and performance as a collection of data in addition to a creative product. Finally, the system outputs the data from a nanoKey2 MIDI keyboard, which is required to use the MIDI file functionality, in a format that SuperCollider can then process and use to create a MIDI file of the performed song. The resultant MIDI file can be opened using applications such as Finale NotePad to show a notated version of the piece that was just played live. This paper describes the work that has been done during the course of the yearlong project. 2.	INITIAL GOALSMy initial goal at the outset of this project was to create a musical system using the programming languages of Processing and SuperCollider in which sounds would be represented visually. Different musical inputs, controlled by the user typing on a computer keyboard or using a MIDI keyboard, were to be interpreted by the system to activate corresponding visual responses. I wanted to analyze musical data in order to determine the specifics of the visual output. For example, I envisioned that perhaps pitches could be mapped to colors (an A might be a blue, A# would be a dark blue, and Ab is a light blue while a B is green, a C orange, etc.). Amplitude could be represented by the saturation of the color, creating a spectrum of shades depending on how forcefully the user was pressing on the keys. Eventually, one goal would be that if a user plays two notes or creates a chord, based on the color scheme, the colors of the respective notes might blend. This description was about as specific as I could get, and in reality I did not begin with much of a vision beyond this. 	I knew that SuperCollider would serve as the ideal language to process the music side of the system, and that SuperCollider could then be linked to the work in Processing, where the graphic displays were to be programmed. Due to an inability to precisely predict how difficult some of these ideas would be to implement, I set several tiers or benchmarks for the project. The first was a base level system with the functionality described above. Both languages are obviously capable of doing far more complex things, so I expected to become more comfortable with the languages in hopes of increasing the capabilities and features of the system. 	Ultimately, the system that I created accomplished those initial goals and built upon them to create something interesting, unique, and useful. There is, of course, room for development and plenty of ways to expand upon the work that I’ve done (see Section 6), but the finished product of this project is satisfying. 3.	FIRST SEMESTER: DECISIONS, DILEMMAS, AND DEVELOPMENTSOnce I’d established SuperCollider and Processing as the ideal languages for the project, I spent the first two weeks of the fall semester diving into those languages and understanding their basic features. In a separate class, CPSC 431, I was learning SuperCollider and beginning to make music with it, so that helped develop a working foundation. 	In Processing, I explored tutorials and developed several test examples that played with different shapes, colors, textures, as well as various other features that Processing provides. I then looked at examples of simple sound synthesis projects in Processing that interact with SynthDefs written in SuperCollider. These were obviously rudimentary, but they got the ball rolling in terms of linking code from both. 	With a foundation solidly laid, it became time to make decisions. This was initially a struggle, as I definitely felt some burden of free will. With so many interesting directions to head, I did not know how to actually pick one path to travel down. Some of the questions I faced: “How will the user interact with the system?” “How will they control the interface?” More broadly, “who is the user? What levels of musical experience do they have?” “What will come first, the vision or the music?” 	It became clear that I wanted a primarily interactive system where the user could play into it in real time, as opposed to one in which the user simply inputs a composition and lets the system do its thing (although I hope to include that feature in the end). This was in part because Processing is incredibly useful for interactivity. Thus, I crafted a piano keyboard using the standard QWERTY keyboard on any computer. Starting with the ‘z’ key on the bottom left, it was easy to match keys into a shape roughly like the staggered black-white keys you find on pianos. Thus, the user’s method of interacting with the system was defined.  	Given that the system would have an actual keyboard to play on, it felt obvious that they would have some experience with music. However, just as anyone can mess around on a keyboard, anyone can play into the system and see what the visual will become. I also felt that the music should come first—the user inputs notes in real time and a vision is born from that, as opposed to the other way around.	Due to the strengths of each respective language, the only code written in SuperCollider thus far is the SynthDef for the piano sound. Everything else is implemented in Processing. 	With these fundamental pillars established, I wrote the most basic version of my actual project, which was based on the idea of a nondeterministic finite state machine. There were several pet examples made before this, but this was the most significant test run. Given an initial musical home state, the tonic, each successive note played by the user would lead to a new state with different options for the next note, leading to a web of relationships between different pitches playable on the keyboard. This would lead to different visual reactions as well. For example, in this version of the project, the home state was a black circle in the middle of the screen. Playing an octave above or below that state reinforces tonic, a move represented visually by increasing the diameter of the circle. However, playing a “dissonant” interval would cause the circle to stretch and also diminished its opacity, thus representing less stability/symmetry. However, playing “consonant” pitches would flatten the ellipse back out and bring it closer to its home state shape. The states came into play because depending on what the previous note had been, that determined the options for possible visual events. For instance, playing the chordal progression of V-I (dominant to tonic) would reset the circle back to the home state. Images 1 and 2 show examples of the relationships between the notes, and how those relationships affect the visualization. This version was basic but gave me a direction in which to head. Simply getting started showed me what worked and what did not work. This code is not included in the submission as it plays no role in the product that I submitted, other than the starting point from which I further developed the system.	After researching a little more and in order to facilitate more clear user understanding of what was going on—of how the notes they were playing changed the visualization—I decided to implement an actual piano keyboard in the interface so that users could see the keys light up as they were playing them and quickly understand which keyboard keys corresponded to which piano keys/pitches. I found examples of keyboards on the Processing Exhibition page and set about making my own (see comments at the top of the code for a description). By using the shift/CAPS lock keys, another octave could be created in addition to the two made from regular keys, so as it stands there are three octaves (because numeric keys do not respond to CAPS lock, I did not add a fourth octave). Creating the image itself was not challenging, although I did struggle to get the correct keys to light up for the correct amount of time when being pressed. It was also more of a challenge to get the mouse functionality (clicking on the key) to work because when the mouse is pressed, you receive little information about what it is clicking on, unlike when a key is pressed (it tells you what the key is!). 	In order to allow the user to manipulate the interface, I added an OFF switch, which kills the sound, frees all synths, and closes all windows. I also added a toggle so that the user can decide whether or not they need the piano on the screen. The music is unaffected, but turning the piano off just makes the entire GUI window dedicated to the visualization.	The most unfamiliar and challenging aspect was linking the sound from SuperCollider to Processing. This came down to finding the right SynthDef. I needed to find a piano-like synth, but the main unit generator used for generating that kind of sound, MdaPiano, had a fatal flaw: because the release of a key played would free that synth, the MdaPiano would kill the sound instantly upon release. But, because piano keys resonate and decay even once the finger has been lifted from the key, I needed a more sustained note. I struggled to find piano SynthDefs that both sounded nice and had this feature (unsuccessful versions are included at the bottom of the SuperCollider file), but was unlucky until stumbling upon one on compositionprogrammer.com. What I found online was not perfect as it was, but with help from Scott, we tweaked it as necessary so that it would perform the way we needed it to.4.	SECOND SEMESTER: VISUALIZATION DEVELOPMENTThis semester I focused almost exclusively on the visual representation aspect of the project. The system was already hooked up to the SuperCollider synths after the work from last semester, so the work for this semester was done in Processing. Given the courses I’ve taken on graph theory and on how quantitative music is, I quickly determined that it would be interesting to depict something creative and performative like live music in a more analytical way that utilized the things I’d learned in other the computer science theory courses I’d taken for the major.	Given that the user of the system can have varying degrees of piano experience, I didn’t want them to be forced to have the piano display. As a result, I created an option for them to hide the piano and free space for a more detailed visualization. This can be done by clicking on the button at the top right of the interface (labeled “Show Piano?”). When the piano is not displayed, the user is shown a chart that depicts the weights of the chords being played in the piece (explained more further down). The user can oscillate back and forth between these two settings.As for the visual, I imagined that each pitch could be represented with a node of its own, and from there it made more sense to position the nodes in a circle, so that each one would have access to all of the others without necessarily needing to cross a different one (which would be the case with different layout styles, like a grid). Image 3 shows what that initial design, which has remained the initial configuration of the visualization through the final version that I am submitting, looked like. Once the pitch is played, the radius of its node grows by a small increment and it begins to travel inwards towards the center, thus making it easy for the user to see which pitches have traveled the most. Those that are closest to the center are typically the ones that have been played the most. Thus, the circular configuration puts each node at the same distance in relation to the center. As for color, the nodes are all initially black. Once a note is played, its node is illuminated into the color that corresponds with its pitch class. Similar to Alexander Scriabin’s Clavier à Lumières (Cohen), each pitch class is represented by a color (see Image 4 for a better look at how colors were assigned). Because I was operating with three octaves, if all nodes were illuminated, each node would have two others with matching colors. 	I recognized that after many nodes have been activated and there are a lot of overlapping parts, it would be difficult to tell which node corresponded to which pitch. For that reason, I added a tracking feature such that when turned on (using the “TRACK” button), users would be able to read the labels attached to each activated node and understand which was which. 	In order to make the system more functional, I added the capability to deal with chords when two or more notes are played simultaneously. The system judges this by taking time stamps of when each is pressed; if only 50 milliseconds have passed between the two most recent notes being played, the computer flags this as a chord. Instead of each node moving towards the center like normal, the nodes begin to move towards each other. In this way, after certain chords are played repeatedly throughout the duration of the piece, the notes that compose those chords are drawn to each other and it becomes clear that there is a relationship between them. In addition, I demonstrated the chord relationships by adding a side panel that is shown when users opt to not display the piano, as explained at the top of this section. The chord chart tracks every chord that is played and indicates those that are most significant. The chart consists of 12 circles, one for each pitch class. As chords are played, they are tallied and the circles that correspond to each element of the chord are enlarged and made more visible. If a chord composes over 20% of all chords played by the user, it is deemed significant and the components of the chord are marked by a smaller circle beneath, in a particular color that matches the other notes of that significant chord. Once the MIDI keyboard was added, significant chords were further weighted by the velocity data. If a note had a high velocity, its significance was considered stronger than a note with a low velocity, as home keys are often played on the stronger beats. Image 4 shows the chord chart and also demonstrates the color assignments that were inspired by but do not match the Clavier à Lumières mentioned above. Another feature added was the path that shows the three most recent notes played. The white path can be seen linking the nodes on the right of the circle in Image 4. The path between the most recent pair of notes has the strongest stroke weight, while the faintest links the third and fourth most recently played notes, so you can see where the melody is coming from.I also added the three-dimensional view feature, which allows users to get a more thorough look at the visualization, once completed, than they would get from a simple two-dimensional form. In order to get this perspective, users can hit the “DONE” button on the top left, which launches into the 3D view. Using the mouse, one can then scroll around to get a look at the image from every angle. I achieved this using the Peasy camera implementation (Feinberg). Image 5 shows what the excerpt from Image 4 looks like under the 3D lens.Finally, I hooked the system up to a NanoKey2 MIDI keyboard, which made the system more practical, as an actual piano keyboard is far easier to play than the configuration on a computer keyboard. The MIDI keyboard also gives more data, like velocity data, that, as mentioned above, was used to determine the significance of certain chords. Using the MIDI keyboard also allowed me to create a MIDI file from the notes played. The finish() function in the Processing code takes all of the MIDI information (pitches, timestamps, note on vs. note off information, and more) and writes it into a text file that is then read by the code written in SuperCollider. Using the SimpleMIDIFile class in SuperCollider, that information is used to create MIDI events that are then strung together into a .mid file. The .mid file is exported and can then be read by notation software to create sheet music for whatever was played. To initiate the creation of a MIDI file, the user pushes “DONE” and then exits out of the application. The final result, then, is a notated version of the piece that was played. One challenge behind this step was that typically MIDI files know the time signature of the piece. Because this system can be used to simply play random melodies and improvise, a time signature is not always known, which leads to a messier notation and requires some fiddling once the software has done its job. This problem is discussed in a little more detail in Section 6. 5.	SETTING UP THE SYSTEM FROM SCRATCHThere are some steps that must be taken in order to properly prepare your machine to run the system. First, both Processing and SuperCollider must be downloaded (this can be done at https://processing.org/download/ and http://supercollider.github.io/download). In bth SuperCollider and Processing, there are some additional libraries and/or quarks that must be installed in order to run the two source files. In Final.scd, the SuperCollider file, under Language -> Quarks, select MathLib and wslib in order to enable the SimpleMIDIFile class to be used. In Processing, under Sketch -> Import Library, search for and select the following: ControlP5, oscP5, the SuperCollider client for Processing, Minim (all for SuperCollider and audio interactions), Jasmine, PeasyCam (for 3D modelling), and TheMidiBus (for MIDI keyboard interactions). Additionally, the code is set up for interface with a nanoKey2 MIDI keyboard, so that will need to be acquired in order to use that functionality. If you have a different MIDI keyboard, line 296 of Final.pde will need to be edited, as that line is coded specifically to identify “KEYBOARD” as the MIDI input that corresponds with the nanoKey2 and “CTRL” is specifically the associated output. Those will need to be replaced with the keys to whatever keyboard is being substituted. You will also need to ensure that file path in like 950 of Final.pde is the appropriate path for your directory. That applies to line 66 and 79 in Final.scd as well. From there, the two source files are ready to be executed, with instructions for that implementation included below. In order to see the notated output of the system, a notation software like Finale NotePad (http://www.finalemusic.com/products/finale-notepad/resources/) needs to be downloaded as well.6.	FINAL DELIVERABLESI am submitting two files, Final.pde (the Processing code) and Final.scd (the SuperCollider code). There are two blocks to run in the SuperCollider file. The first is the SynthDef at the top of the file, which should be run first. The Processing code is run second, and then the second block in the SuperCollider code, which creates the MIDI file, is executed. This should be run after the “DONE” button is pushed on the Processing interface. The only material needed beyond a computer to run the code is a nanoKey2 MIDI keyboard. The steps are:1.	SuperCollider: Boot the server and run “SynPiano” SynthDef block (the first block in Final.scd)2.	Processing: Run the entirety of Final.pde3.	Press “DONE” on the interface to see the 3D visualization and initiate the MIDI file creation4.	SuperCollider: “MIDI File Creation” block (the second block in Final.scd)My project requires the user to play a piece into the computer keyboard and/or MIDI keyboard. From what the user plays, a visualization of that music is created that highlights the major chords and progressions of the piece, giving some insight into the key features of the composition in a unique and insightful way. If the user has a MIDI keyboard at hand, they have the liberty to not only play a piece, but to create a MIDI file that can give them a notated version of the piece. That is useful if the user is improvising or composing by ear, and not necessarily playing from sheet music, and wants a copy of what was played. Ultimately, this project has proven to be an exciting venture into music data analytics, and a comprehensive lesson in both Processing and SuperCollider programming languages. 7.	MOVING FORWARDThere is room to develop this project so that it can accommodate more complex musical features, as it currently does not capture intricacies understood through more involved music theory. The visuals created by the program would thus be more detailed and nuanced. The system could also be expanded to account for multiple tracks, whereas right now it can only handle one. The MIDI file that is output could also be more precise so that the notation software can more easily interpret it. As of now, the tempo and key of the song cannot be specified by the information presented, so the notation generated by the software does not have an accurate time or key signature. While it isn’t too hard to deduce that information from what is given, a further development could be to use the timestamps to identify the time signature before creating the MIDI file. That way, the bars all line up the way they should and no shuffling needs to be done by the user once the notation software does its work. It is clear that this is a prototype for something even more useful and insightful that can be developed moving forward. 8.	ACKNOWLEDGEMENTSThe work this year would not have been completed without the assistance from Professor Scott Petersen, whose guidance pushed me in the right direction and whose SuperCollider experience (his SynthDef wizardry, in particular) helped create the piano synth that was ultimately used, after much exploration in this area. Professor Holly Rushmeier was also instrumental in keeping me on track during the first semester and ensuring that the work was evenly distributed throughout that term. Thank you as well to Professor Julie Dorsey for advising me through the Computing and the Arts program, and to my professors in both the Music and Computer Science departments for the all of the time, wisdom, and learning during my four years at Yale. 9.	RESOURCESTextbook Sources:•	Fry, Ben. Visualizing Data: Exploring and Explaining Data with the Processing Environment. O’Reilly Media, Sebastopol, CA, 2008. •	Fry, Ben & Casey Reas. Processing: A Programming Handbook for Visual Designers and Artists. Massachusetts Institute of Technology Press, Cambridge, MA, 2007. •	Valle, Andrea. Introduction to SuperCollider. Logos Verlag Berlin, Berlin, 2016.Online Sources for Research and Inspiration:•	“Algorithmic Design for the Creative Hive.” Open Processing. 2016, www.openprocessing.org.•	Cohen, Josh. “Alexander Scriabin and Artificial Synesthesia”. Art of Memory, http://artofmemory.com/forums/alexander-scriabin-and-artificial-synesthesia-1016.html. •	 “Processing Exhibition Archive: a curated collection of projects created with Processing.” Processing.org, https://www.processing.org/exhibition/.•	“Processing.js Exhibition.” Processing.js, http://processingjs.org/exhibition/.Coding Resources:•	Collins, Nick. “Physical Modelling Synthesis.” Composer Programmer, Durham University. https://composerprogrammer.com/teaching/supercollider/sctutorial/11.1%20Physical%20Modelling.html. •	Feinberg, Jonathan. “peasycam v202.” http://mrfeinberg.com/peasycam/. •	Smith, Severin. “The MidiBus: A MIDI Library for Processing.” Small but Digital. http://www.smallbutdigital.com/projects/themidibus/. 